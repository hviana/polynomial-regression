Implement TypeScript library "ConvolutionalRegression": convolutional neural network for multivariate regression with incremental online learning, Adam optimizer, z-score normalization.

PERFORMANCE:
- Minimize memory allocations by reusing arrays and objects wherever possible
- Use typed arrays (Float64Array) for all numerical computations
- Avoid creating intermediate arrays in hot paths
- Implement in-place matrix operations
- Use object pooling for frequently created objects
- Minimize garbage collection pressure
- Cache computed values that are reused
- Use efficient loop structures (avoid forEach, map, reduce in critical paths)
- Preallocate buffers for convolution operations and layer activations
- Implement lazy initialization where appropriate
- Use iterative methods to prevent memory overallocation
- Write code that is highly CPU-optimized as well

DESIGN:
- Full numerical stability; OOP with interfaces for public contracts
- Private field encapsulation; JSDoc (@param, @returns, @example)
- Inline math formula docs; normalize inputs; track running average loss for accuracy

CONFIG (defaults):
hiddenLayers: 2 (1-10), convolutionsPerLayer: 32 (1-256), kernelSize: 3, learningRate: 0.001, warmupSteps: 100, totalSteps: 10000, beta1: 0.9, beta2: 0.999, epsilon: 1e-8, regularizationStrength: 1e-4, batchSize: 32, convergenceThreshold: 1e-6, outlierThreshold: 3.0, adwinDelta: 0.002

API:
fitOnline({ xCoordinates: number[][], yCoordinates: number[][] }): FitResult → Incremental Adam, Welford's z-score, L2 reg, outlier downweighting, ADWIN drift detection
fitBatch({ xCoordinates: number[][], yCoordinates: number[][], epochs?: number }): BatchFitResult → Mini-batch GD, shuffling, early stopping
predict(futureSteps: number): PredictionResult → Predictions
getModelSummary(): ModelSummary | getWeights(): WeightInfo | getNormalizationStats(): NormalizationStats | reset(): void

TYPES:
FitResult { loss, gradientNorm, effectiveLearningRate, isOutlier, converged, sampleIndex, driftDetected }
BatchFitResult { finalLoss, lossHistory: number[], converged, epochsCompleted, totalSamplesProcessed }
PredictionResult { predictions: SinglePrediction[], accuracy, sampleCount, isModelReady }
SinglePrediction { predicted, lowerBound, upperBound, standardError: number[] }
WeightInfo { kernels, biases, firstMoment, secondMoment: number[][][], updateCount }
NormalizationStats { inputMean, inputStd, outputMean, outputStd: number[], count }
ModelSummary { isInitialized, inputDimension, outputDimension, hiddenLayers, convolutionsPerLayer, kernelSize, totalParameters, sampleCount, accuracy, converged, effectiveLearningRate, driftCount }

ALGORITHMS:

Convolutional Network Architecture:
1. Auto-detect: inputDim = xCoordinates[0].length, outputDim = yCoordinates[0].length
2. Structure: Input(inputDim) → [Conv1D(convolutionsPerLayer, kernelSize, padding='same') → Activation]×hiddenLayers → Flatten → Dense(outputDim)
3. Conv1D: y[c,i] = Σₖ Σⱼ(W[c,k,j] · x[k,i+j-pad]) + b[c], same padding preserves spatial dims
4. Activation: ReLU(x) = max(0, x), applied element-wise

Adam + Cosine Warmup:
1. Normalize: x̃ = (x - μ)/(σ + ε)
2. Forward: propagate through conv layers, cache activations for backprop
3. Loss: L = (1/2n)Σ‖y - ŷ‖² + (λ/2)Σ‖W‖²
4. Backprop: ∂L/∂W via chain rule, ∂Conv: rotate kernel, convolve with upstream gradient
5. LR: warmup → cosine decay
6. Update: m = β₁m + (1-β₁)g, v = β₂v + (1-β₂)g², W -= η(m/(1-β₁ᵗ))/(√(v/(1-β₂ᵗ)) + ε)

Welford: δ = x - μ, μ += δ/n, M₂ += δ(x - μ), σ² = M₂/(n-1) → inputs (normalize)

Accuracy: track running average loss L̄ = ΣLoss/n; accuracy = 1/(1 + L̄)

ADWIN Drift: adaptive error window; detect drift when |μ₀ - μ₁| ≥ εcut(δ); shrink window; reset stats on drift

Outliers: r = (y - ŷ)/σ; |r| > threshold → downweight 0.1×

Export: ConvolutionalRegression
