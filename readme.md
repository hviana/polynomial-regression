> ## âš ï¸ğŸš¨ IMPORTANT: THIS LIBRARY HAS BEEN DEPRECATED ğŸš¨âš ï¸
> 
> ---
> 
> ### ğŸ”„ This library has been replaced by a newer, more powerful version!
> 
> <table>
> <tr>
> <td>
> 
> ### âŒ OLD (This Repository)
> `@hviana/polynomial-regression`
> 
> </td>
> <td>
> 
> ### âœ… NEW (Use This Instead)
> `@hviana/multivariate-convolutional-regression`
> 
> </td>
> </tr>
> </table>
> 
> ---
> 
> ### ğŸ“¦ Migration Links
> 
> | Platform | Link |
> |----------|------|
> | ğŸŒ **JSR Registry** | ğŸ‘‰ [https://jsr.io/@hviana/multivariate-convolutional-regression](https://jsr.io/@hviana/multivariate-convolutional-regression) |
> | ğŸ™ **GitHub Repository** | ğŸ‘‰ [https://github.com/hviana/multivariate-convolutional-regression](https://github.com/hviana/multivariate-convolutional-regression) |
> 
> ---
> 
> ### ğŸ›‘ Please migrate to the new library for:
> - âœ¨ New features and improvements
> - ğŸ› Bug fixes and security updates
> - ğŸ“š Better documentation
> - ğŸ”§ Continued maintenance and support
> 
> ---

Model: # ğŸš€ Multivariate Polynomial Regression

**A high-performance TypeScript library for multivariate polynomial regression
with incremental online learning, Adam optimizer, and z-score normalization.**

---

## ğŸ“‹ Table of Contents

- [Key Advantages](#-key-advantages)
- [Quick Start](#-quick-start)
- [API Reference](#-api-reference)
- [Configuration Parameters](#-configuration-parameters)
- [Use Case Optimization Guide](#-use-case-optimization-guide)
- [Mathematical Background](#-mathematical-background)
- [Performance Tips](#-performance-tips)

---

## âœ¨ Key Advantages

### ğŸ¯ **Core Strengths**

| Feature                           | Benefit                                                                       |
| --------------------------------- | ----------------------------------------------------------------------------- |
| ğŸ”„ **Online Learning**            | Incrementally update your model with new data without retraining from scratch |
| ğŸ“ˆ **Polynomial Features**        | Automatically captures non-linear relationships up to degree 10               |
| âš¡ **Adam Optimizer**             | State-of-the-art optimization with adaptive learning rates                    |
| ğŸ›¡ï¸ **Robust to Outliers**         | Built-in z-score outlier detection and downweighting                          |
| ğŸŒŠ **Concept Drift Detection**    | ADWIN algorithm automatically detects and adapts to data distribution changes |
| ğŸ“Š **Uncertainty Quantification** | Predictions include confidence intervals and standard errors                  |

### ğŸ† **Performance Features**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ§® Float64Array          â”‚  Maximum numerical precision        â”‚
â”‚  ğŸ” Object Pooling        â”‚  Minimized garbage collection       â”‚
â”‚  ğŸ“¦ Buffer Preallocation  â”‚  Zero-allocation hot paths          â”‚
â”‚  ğŸ² Xavier Initialization â”‚  Optimal gradient flow              â”‚
â”‚  ğŸ“‰ Cosine LR Decay       â”‚  Smooth convergence                 â”‚
â”‚  ğŸ”¢ Welford's Algorithm   â”‚  Numerically stable statistics      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¨ **Why Choose This Library?**

1. **ğŸ”¥ Production-Ready** - Handles edge cases, validates inputs, and provides
   meaningful error messages
2. **ğŸ“± Memory Efficient** - Object pooling and buffer reuse minimize memory
   footprint
3. **ğŸ›ï¸ Highly Configurable** - 12 tunable parameters for fine-grained control
4. **ğŸ“– Self-Documenting** - Rich TypeScript interfaces with comprehensive JSDoc
5. **ğŸ”„ Dual Training Modes** - Both online (streaming) and batch training
   supported
6. **ğŸ“Š Full Observability** - Access to weights, normalization stats, and model
   summary

---

## ğŸš€ Quick Start

### Basic Usage

```typescript
import { MultivariatePolynomialRegression } from "jsr:@hviana/polynomial-regression@1.1.0";

// 1ï¸âƒ£ Create a model
const model = new MultivariatePolynomialRegression({
  polynomialDegree: 2,
  learningRate: 0.01,
});

// 2ï¸âƒ£ Train with batch data
const result = model.fitBatch({
  xCoordinates: [
    [1, 2],
    [2, 3],
    [3, 4],
    [4, 5],
    [5, 6],
  ],
  yCoordinates: [
    [5],
    [8],
    [13],
    [20],
    [29],
  ],
  epochs: 100,
});

console.log(`âœ… Training complete! Final loss: ${result.finalLoss}`);

// 3ï¸âƒ£ Make predictions
const predictions = model.predict(3);
predictions.predictions.forEach((p, i) => {
  console.log(
    `Step ${i + 1}: ${p.predicted[0].toFixed(2)} Â± ${
      p.standardError[0].toFixed(2)
    }`,
  );
});
```

### Online Learning (Streaming Data)

```typescript
const model = new MultivariatePolynomialRegression();

// Stream data point by point
for (const dataPoint of dataStream) {
  const result = model.fitOnline({
    xCoordinates: [dataPoint.x],
    yCoordinates: [dataPoint.y],
  });

  if (result.driftDetected) {
    console.log("âš ï¸ Concept drift detected! Model adapting...");
  }

  if (result.converged) {
    console.log("âœ… Model converged!");
    break;
  }
}
```

---

## ğŸ“š API Reference

### Constructor

```typescript
const model = new MultivariatePolynomialRegression(config?: MultivariatePolynomialRegressionConfig);
```

### Methods

| Method                    | Description                            | Returns              |
| ------------------------- | -------------------------------------- | -------------------- |
| `fitOnline(input)`        | Incremental learning on streaming data | `FitResult`          |
| `fitBatch(input)`         | Batch training with mini-batches       | `BatchFitResult`     |
| `predict(steps)`          | Generate predictions with uncertainty  | `PredictionResult`   |
| `getModelSummary()`       | Get model state overview               | `ModelSummary`       |
| `getWeights()`            | Access weight matrices                 | `WeightInfo`         |
| `getNormalizationStats()` | Get normalization parameters           | `NormalizationStats` |
| `reset()`                 | Reset model to initial state           | `void`               |

---

## âš™ï¸ Configuration Parameters

### Overview

```typescript
interface MultivariatePolynomialRegressionConfig {
  polynomialDegree?: number; // Feature expansion degree
  learningRate?: number; // Base learning rate
  warmupSteps?: number; // LR warmup period
  totalSteps?: number; // Total training steps
  beta1?: number; // Adam Î²â‚ parameter
  beta2?: number; // Adam Î²â‚‚ parameter
  epsilon?: number; // Numerical stability
  regularizationStrength?: number; // L2 regularization
  batchSize?: number; // Mini-batch size
  convergenceThreshold?: number; // Early stopping threshold
  outlierThreshold?: number; // Outlier z-score threshold
  adwinDelta?: number; // Drift detection sensitivity
}
```

---

### 1ï¸âƒ£ **polynomialDegree**

> ğŸ“ Controls the complexity of feature expansion

| Property    | Value    |
| ----------- | -------- |
| **Type**    | `number` |
| **Range**   | 1 - 10   |
| **Default** | 2        |

#### ğŸ“– Explanation

The polynomial degree determines how many polynomial features are generated from
your input. For inputs with `n` dimensions and degree `d`, the number of
features is:

$$C(n+d, d) = \frac{(n+d)!}{n! \cdot d!}$$

**Feature expansion example for 2D input `[xâ‚, xâ‚‚]`:**

| Degree | Features Generated                                  | Count |
| ------ | --------------------------------------------------- | ----- |
| 1      | `1, xâ‚, xâ‚‚`                                         | 3     |
| 2      | `1, xâ‚, xâ‚‚, xâ‚Â², xâ‚xâ‚‚, xâ‚‚Â²`                         | 6     |
| 3      | `1, xâ‚, xâ‚‚, xâ‚Â², xâ‚xâ‚‚, xâ‚‚Â², xâ‚Â³, xâ‚Â²xâ‚‚, xâ‚xâ‚‚Â², xâ‚‚Â³` | 10    |

#### ğŸ’¡ Examples

```typescript
// ğŸ”µ Linear relationships (simple, fast)
const linearModel = new MultivariatePolynomialRegression({
  polynomialDegree: 1,
});

// ğŸŸ¢ Quadratic patterns (parabolas, ellipses)
const quadraticModel = new MultivariatePolynomialRegression({
  polynomialDegree: 2,
});

// ğŸŸ  Complex non-linear patterns
const complexModel = new MultivariatePolynomialRegression({
  polynomialDegree: 4,
});
```

#### ğŸ¯ Optimization Guide

| Use Case                | Recommended Degree | Rationale                      |
| ----------------------- | ------------------ | ------------------------------ |
| Linear trends           | 1                  | Minimal complexity, fastest    |
| Quadratic curves        | 2                  | Captures curvature efficiently |
| Sensor data             | 2-3                | Balances accuracy and speed    |
| Physics simulations     | 3-4                | Captures complex dynamics      |
| High-precision modeling | 4-6                | When accuracy is paramount     |
| Research/exploration    | 6-10               | Maximum flexibility            |

> âš ï¸ **Warning**: Higher degrees exponentially increase feature count and
> training time. A 10D input with degree 5 produces 3,003 features!

---

### 2ï¸âƒ£ **learningRate**

> ğŸšï¸ Controls the step size during optimization

| Property    | Value    |
| ----------- | -------- |
| **Type**    | `number` |
| **Range**   | > 0      |
| **Default** | 0.001    |

#### ğŸ“– Explanation

The learning rate determines how much weights are updated in response to
gradients:

```
W_new = W_old - learningRate Ã— gradient
```

The library uses **cosine decay with warmup**:

- **Warmup**: `Î·_t = Î· Ã— t / warmupSteps`
- **Decay**: `Î·_t = Î· Ã— 0.5 Ã— (1 + cos(Ï€ Ã— progress))`

#### ğŸ’¡ Examples

```typescript
// ğŸ¢ Conservative learning (stable but slow)
const conservativeModel = new MultivariatePolynomialRegression({
  learningRate: 0.0001,
});

// ğŸš¶ Standard learning rate
const standardModel = new MultivariatePolynomialRegression({
  learningRate: 0.001,
});

// ğŸƒ Aggressive learning (fast but may overshoot)
const aggressiveModel = new MultivariatePolynomialRegression({
  learningRate: 0.01,
});

// ğŸš€ Very fast initial convergence
const fastModel = new MultivariatePolynomialRegression({
  learningRate: 0.1,
  warmupSteps: 200, // Important: longer warmup for stability
});
```

#### ğŸ¯ Optimization Guide

| Scenario                     | Recommended LR | Notes                       |
| ---------------------------- | -------------- | --------------------------- |
| Small dataset (<100 samples) | 0.01 - 0.1     | Faster convergence needed   |
| Medium dataset (100-10K)     | 0.001 - 0.01   | Standard range              |
| Large dataset (>10K)         | 0.0001 - 0.001 | Stability over speed        |
| Online/streaming             | 0.001 - 0.01   | Balance adaptation speed    |
| High polynomial degree       | 0.0001 - 0.001 | Prevent exploding gradients |
| Noisy data                   | 0.0001 - 0.001 | Smoother updates            |

---

### 3ï¸âƒ£ **warmupSteps**

> ğŸŒ¡ï¸ Gradual learning rate increase at start

| Property    | Value              |
| ----------- | ------------------ |
| **Type**    | `number` (integer) |
| **Range**   | â‰¥ 0                |
| **Default** | 100                |

#### ğŸ“– Explanation

During warmup, the learning rate linearly increases from near-zero to the full
rate:

```
Î·_t = learningRate Ã— (t + 1) / warmupSteps
```

This prevents large initial weight updates when statistics are uncertain.

```
Learning Rate
     ^
  Î·  |           â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
     |          â•±                â•²  â† Cosine decay
     |         â•±                  â•²
     |        â•±                    â•²
     |   â•±â”€â”€â”€â•¯                      â•²
   0 |â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Steps
       â†‘           â†‘
    Warmup     Full rate
```

#### ğŸ’¡ Examples

```typescript
// ğŸ”¥ No warmup (aggressive start)
const noWarmup = new MultivariatePolynomialRegression({
  warmupSteps: 0,
});

// ğŸŒ¤ï¸ Quick warmup (standard)
const quickWarmup = new MultivariatePolynomialRegression({
  warmupSteps: 50,
});

// ğŸŒ… Gradual warmup (safe)
const gradualWarmup = new MultivariatePolynomialRegression({
  warmupSteps: 200,
});

// ğŸŒ„ Extended warmup (very stable)
const extendedWarmup = new MultivariatePolynomialRegression({
  warmupSteps: 500,
  learningRate: 0.01, // Can use higher LR with longer warmup
});
```

#### ğŸ¯ Optimization Guide

| Scenario              | Recommended Steps | Notes                             |
| --------------------- | ----------------- | --------------------------------- |
| Quick experimentation | 0-20              | Speed over stability              |
| Standard training     | 50-100            | Good balance                      |
| High learning rate    | 100-300           | Prevents initial instability      |
| Large batches         | 50-100            | Statistics stabilize faster       |
| Online learning       | 100-200           | Gives time for stats to stabilize |
| Production systems    | 100-500           | Maximum stability                 |

---

### 4ï¸âƒ£ **totalSteps**

> ğŸ“ Total steps for learning rate schedule

| Property    | Value              |
| ----------- | ------------------ |
| **Type**    | `number` (integer) |
| **Range**   | > 0                |
| **Default** | 10000              |

#### ğŸ“– Explanation

`totalSteps` defines the full cosine decay schedule length. After `totalSteps`,
the learning rate approaches zero.

```
Effective LR = baseLR Ã— 0.5 Ã— (1 + cos(Ï€ Ã— (step - warmup) / (total - warmup)))
```

#### ğŸ’¡ Examples

```typescript
// ğŸ“Š Short training (quick experiments)
const shortTraining = new MultivariatePolynomialRegression({
  totalSteps: 1000,
  warmupSteps: 50,
});

// ğŸ“ˆ Standard training
const standardTraining = new MultivariatePolynomialRegression({
  totalSteps: 10000,
  warmupSteps: 100,
});

// ğŸ“‰ Extended training (complex patterns)
const extendedTraining = new MultivariatePolynomialRegression({
  totalSteps: 100000,
  warmupSteps: 1000,
});

// â™¾ï¸ Continuous online learning
const onlineLearning = new MultivariatePolynomialRegression({
  totalSteps: 1000000, // Very slow decay
  warmupSteps: 1000,
});
```

#### ğŸ¯ Optimization Guide

| Training Mode         | Recommended Steps | Notes                    |
| --------------------- | ----------------- | ------------------------ |
| Batch (small data)    | 1,000 - 5,000     | Faster convergence       |
| Batch (large data)    | 10,000 - 50,000   | Full dataset exploration |
| Online (session)      | 5,000 - 20,000    | Typical session length   |
| Online (continuous)   | 100,000+          | Long-running systems     |
| Hyperparameter search | 500 - 2,000       | Quick evaluation         |

---

### 5ï¸âƒ£ **beta1**

> ğŸ“Š Adam first moment decay rate

| Property    | Value    |
| ----------- | -------- |
| **Type**    | `number` |
| **Range**   | [0, 1]   |
| **Default** | 0.9      |

#### ğŸ“– Explanation

Î²â‚ controls the exponential moving average of gradients (momentum):

```
m_t = Î²â‚ Ã— m_{t-1} + (1 - Î²â‚) Ã— g_t
```

Higher values = more momentum = smoother but slower adaptation.

#### ğŸ’¡ Examples

```typescript
// ğŸ¯ Standard momentum
const standardMomentum = new MultivariatePolynomialRegression({
  beta1: 0.9,
});

// ğŸŒŠ High momentum (smoother updates)
const highMomentum = new MultivariatePolynomialRegression({
  beta1: 0.95,
});

// âš¡ Low momentum (faster adaptation)
const lowMomentum = new MultivariatePolynomialRegression({
  beta1: 0.8,
});

// ğŸ”„ Very low momentum (online learning)
const adaptiveMomentum = new MultivariatePolynomialRegression({
  beta1: 0.5, // Quick response to changes
});
```

#### ğŸ¯ Optimization Guide

| Scenario               | Recommended Î²â‚ | Rationale           |
| ---------------------- | -------------- | ------------------- |
| Standard training      | 0.9            | Well-tested default |
| Noisy gradients        | 0.95 - 0.99    | More smoothing      |
| Concept drift expected | 0.5 - 0.8      | Faster adaptation   |
| Sparse updates         | 0.9 - 0.95     | Maintain momentum   |
| Fine-tuning            | 0.85 - 0.9     | Balanced response   |

---

### 6ï¸âƒ£ **beta2**

> ğŸ“ˆ Adam second moment decay rate

| Property    | Value    |
| ----------- | -------- |
| **Type**    | `number` |
| **Range**   | [0, 1]   |
| **Default** | 0.999    |

#### ğŸ“– Explanation

Î²â‚‚ controls the exponential moving average of squared gradients (variance):

```
v_t = Î²â‚‚ Ã— v_{t-1} + (1 - Î²â‚‚) Ã— g_tÂ²
```

This enables per-parameter learning rates, crucial for Adam's adaptivity.

#### ğŸ’¡ Examples

```typescript
// ğŸ¯ Standard variance tracking
const standard = new MultivariatePolynomialRegression({
  beta2: 0.999,
});

// ğŸ“Š Faster variance adaptation
const fasterVariance = new MultivariatePolynomialRegression({
  beta2: 0.99,
});

// ğŸ”¬ Very stable variance estimate
const stableVariance = new MultivariatePolynomialRegression({
  beta2: 0.9999,
});
```

#### ğŸ¯ Optimization Guide

| Scenario        | Recommended Î²â‚‚ | Rationale                 |
| --------------- | -------------- | ------------------------- |
| General use     | 0.999          | Default works well        |
| Sparse features | 0.999 - 0.9999 | Stable estimates          |
| Dense features  | 0.99 - 0.999   | Faster adaptation         |
| Online learning | 0.99 - 0.999   | Balance stability/speed   |
| Long training   | 0.999 - 0.9999 | Prevent late-stage issues |

---

### 7ï¸âƒ£ **epsilon**

> ğŸ”¢ Numerical stability constant

| Property    | Value    |
| ----------- | -------- |
| **Type**    | `number` |
| **Range**   | > 0      |
| **Default** | 1e-8     |

#### ğŸ“– Explanation

Epsilon prevents division by zero in the Adam update:

```
W -= lr Ã— mÌ‚ / (âˆšvÌ‚ + Îµ)
```

Also used in normalization to prevent division by zero standard deviation.

#### ğŸ’¡ Examples

```typescript
// ğŸ¯ Standard precision
const standard = new MultivariatePolynomialRegression({
  epsilon: 1e-8,
});

// ğŸ”¬ High precision (for well-scaled data)
const highPrecision = new MultivariatePolynomialRegression({
  epsilon: 1e-10,
});

// ğŸ›¡ï¸ Safer for mixed-precision
const safer = new MultivariatePolynomialRegression({
  epsilon: 1e-7,
});

// ğŸ”§ Very safe (noisy/unstable data)
const verySafe = new MultivariatePolynomialRegression({
  epsilon: 1e-6,
});
```

#### ğŸ¯ Optimization Guide

| Scenario                | Recommended Îµ | Notes              |
| ----------------------- | ------------- | ------------------ |
| Standard Float64        | 1e-8          | Default is optimal |
| Large gradient variance | 1e-7 - 1e-6   | More stability     |
| Small gradients         | 1e-9 - 1e-8   | Better precision   |
| Mixed precision         | 1e-6 - 1e-7   | Prevent underflow  |

> ğŸ’¡ **Tip**: Rarely needs adjustment. Change only if you see NaN or Infinity
> values.

---

### 8ï¸âƒ£ **regularizationStrength**

> ğŸ›ï¸ L2 regularization (weight decay) strength

| Property    | Value    |
| ----------- | -------- |
| **Type**    | `number` |
| **Range**   | â‰¥ 0      |
| **Default** | 1e-4     |

#### ğŸ“– Explanation

L2 regularization adds a penalty term to prevent overfitting:

```
Loss = MSE + (Î»/2) Ã— ||W||Â²
Gradient += Î» Ã— W
```

Higher values push weights toward zero, reducing model complexity.

#### ğŸ’¡ Examples

```typescript
// âŒ No regularization (risk of overfitting)
const noReg = new MultivariatePolynomialRegression({
  regularizationStrength: 0,
});

// ğŸ¯ Light regularization (default)
const lightReg = new MultivariatePolynomialRegression({
  regularizationStrength: 1e-4,
});

// ğŸ›¡ï¸ Moderate regularization
const moderateReg = new MultivariatePolynomialRegression({
  regularizationStrength: 1e-3,
});

// ğŸ”’ Strong regularization (prevent overfitting)
const strongReg = new MultivariatePolynomialRegression({
  regularizationStrength: 1e-2,
});

// ğŸ’ª Very strong regularization
const veryStrongReg = new MultivariatePolynomialRegression({
  regularizationStrength: 0.1,
});
```

#### ğŸ¯ Optimization Guide

| Scenario               | Recommended Î» | Notes                               |
| ---------------------- | ------------- | ----------------------------------- |
| Large dataset          | 0 - 1e-5      | Less regularization needed          |
| Small dataset          | 1e-3 - 1e-2   | Prevent overfitting                 |
| High polynomial degree | 1e-3 - 1e-2   | More features = more regularization |
| Noisy data             | 1e-3 - 1e-2   | Smooth out noise                    |
| Clean data             | 1e-5 - 1e-4   | Preserve signal                     |
| Online learning        | 1e-4 - 1e-3   | Stabilize updates                   |

**Rule of thumb**: `Î» âˆ degreeÂ² / dataset_size`

---

### 9ï¸âƒ£ **batchSize**

> ğŸ“¦ Mini-batch size for batch training

| Property    | Value              |
| ----------- | ------------------ |
| **Type**    | `number` (integer) |
| **Range**   | > 0                |
| **Default** | 32                 |

#### ğŸ“– Explanation

During batch training, data is processed in mini-batches:

```
for each epoch:
    shuffle data
    for batch in batches(data, batchSize):
        accumulate gradients over batch
        update weights
```

| Batch Size     | Gradient Quality | Memory | Speed  |
| -------------- | ---------------- | ------ | ------ |
| Small (8-16)   | Noisy            | Low    | Slow   |
| Medium (32-64) | Balanced         | Medium | Medium |
| Large (128+)   | Smooth           | High   | Fast   |

#### ğŸ’¡ Examples

```typescript
// ğŸ”¬ Small batches (more noise, better generalization)
const smallBatch = new MultivariatePolynomialRegression({
  batchSize: 8,
  learningRate: 0.001, // Lower LR for stability
});

// ğŸ¯ Standard batch size
const standardBatch = new MultivariatePolynomialRegression({
  batchSize: 32,
});

// ğŸ“Š Large batches (smoother gradients)
const largeBatch = new MultivariatePolynomialRegression({
  batchSize: 128,
  learningRate: 0.004, // Can scale up LR
});

// ğŸš€ Full batch gradient descent
const fullBatch = new MultivariatePolynomialRegression({
  batchSize: 10000, // If dataset fits
});
```

#### ğŸ¯ Optimization Guide

| Scenario                 | Recommended Size | Notes                    |
| ------------------------ | ---------------- | ------------------------ |
| Small dataset (<100)     | 8 - 16           | More updates per epoch   |
| Medium dataset           | 32 - 64          | Good balance             |
| Large dataset            | 64 - 256         | Leverage parallelism     |
| High noise               | 16 - 32          | Regularization via noise |
| Quick convergence needed | 64 - 128         | Smoother gradients       |
| Memory constrained       | 8 - 32           | Minimize footprint       |

**Linear scaling rule**: When doubling batch size, consider increasing learning
rate by ~âˆš2.

---

### ğŸ”Ÿ **convergenceThreshold**

> ğŸ¯ Early stopping threshold

| Property    | Value    |
| ----------- | -------- |
| **Type**    | `number` |
| **Range**   | > 0      |
| **Default** | 1e-6     |

#### ğŸ“– Explanation

Training stops early when loss change falls below threshold:

```
if |loss_{t} - loss_{t-1}| < convergenceThreshold:
    stop training (converged)
```

Also uses patience mechanism: stops after 10 epochs without improvement.

#### ğŸ’¡ Examples

```typescript
// ğŸ”¬ High precision (train longer)
const highPrecision = new MultivariatePolynomialRegression({
  convergenceThreshold: 1e-8,
});

// ğŸ¯ Standard precision
const standard = new MultivariatePolynomialRegression({
  convergenceThreshold: 1e-6,
});

// âš¡ Quick convergence (stop early)
const quick = new MultivariatePolynomialRegression({
  convergenceThreshold: 1e-4,
});

// ğŸš€ Very quick (fast experiments)
const veryQuick = new MultivariatePolynomialRegression({
  convergenceThreshold: 1e-3,
});
```

#### ğŸ¯ Optimization Guide

| Scenario              | Recommended Threshold | Notes             |
| --------------------- | --------------------- | ----------------- |
| Production model      | 1e-7 - 1e-6           | High quality      |
| Standard training     | 1e-6 - 1e-5           | Good balance      |
| Hyperparameter search | 1e-4 - 1e-3           | Quick evaluation  |
| Real-time systems     | 1e-4                  | Faster adaptation |
| Scientific computing  | 1e-8 - 1e-7           | Maximum precision |

---

### 1ï¸âƒ£1ï¸âƒ£ **outlierThreshold**

> ğŸš¨ Z-score threshold for outlier detection

| Property    | Value    |
| ----------- | -------- |
| **Type**    | `number` |
| **Range**   | > 0      |
| **Default** | 3.0      |

#### ğŸ“– Explanation

Samples with residual z-scores above threshold are downweighted:

```
z_score = |residual - mean_residual| / std_residual
if z_score > threshold:
    sample_weight = 0.1  // Downweight outlier
else:
    sample_weight = 1.0  // Normal weight
```

| Threshold | % Data as Outliers (Normal Distribution) |
| --------- | ---------------------------------------- |
| 2.0       | ~4.6%                                    |
| 2.5       | ~1.2%                                    |
| 3.0       | ~0.3%                                    |
| 3.5       | ~0.05%                                   |

#### ğŸ’¡ Examples

```typescript
// ğŸš¨ Aggressive outlier detection
const aggressive = new MultivariatePolynomialRegression({
  outlierThreshold: 2.0,
});

// ğŸ¯ Standard detection
const standard = new MultivariatePolynomialRegression({
  outlierThreshold: 3.0,
});

// ğŸ›¡ï¸ Conservative detection
const conservative = new MultivariatePolynomialRegression({
  outlierThreshold: 4.0,
});

// âŒ Disable outlier detection
const noOutlierDetection = new MultivariatePolynomialRegression({
  outlierThreshold: 100, // Effectively disabled
});
```

#### ğŸ¯ Optimization Guide

| Data Quality       | Recommended Threshold | Notes                |
| ------------------ | --------------------- | -------------------- |
| Clean data         | 3.5 - 4.0             | Minimal intervention |
| Some noise         | 3.0                   | Default handles well |
| Noisy data         | 2.5 - 3.0             | More aggressive      |
| Many outliers      | 2.0 - 2.5             | Strong filtering     |
| Outliers are valid | 4.0+                  | Don't discard        |

> âš ï¸ **Note**: Outlier detection only activates after 20 samples (needs
> statistics).

---

### 1ï¸âƒ£2ï¸âƒ£ **adwinDelta**

> ğŸŒŠ ADWIN drift detection sensitivity

| Property    | Value    |
| ----------- | -------- |
| **Type**    | `number` |
| **Range**   | (0, 1)   |
| **Default** | 0.002    |

#### ğŸ“– Explanation

ADWIN (ADaptive WINdowing) detects concept drift by comparing window means:

```
Drift detected when: |Î¼_left - Î¼_right| â‰¥ âˆš((2/m) Ã— ln(2/Î´))
```

Where Î´ (delta) controls sensitivity:

- **Smaller Î´** = More sensitive = More false positives
- **Larger Î´** = Less sensitive = May miss drift

#### ğŸ’¡ Examples

```typescript
// ğŸ”¬ Highly sensitive (catch small changes)
const sensitive = new MultivariatePolynomialRegression({
  adwinDelta: 0.0001,
});

// ğŸ¯ Standard sensitivity
const standard = new MultivariatePolynomialRegression({
  adwinDelta: 0.002,
});

// ğŸ›¡ï¸ Conservative (only major shifts)
const conservative = new MultivariatePolynomialRegression({
  adwinDelta: 0.01,
});

// ğŸ˜´ Low sensitivity (stable environments)
const lowSensitivity = new MultivariatePolynomialRegression({
  adwinDelta: 0.1,
});
```

#### ğŸ¯ Optimization Guide

| Environment         | Recommended Î´  | Notes                 |
| ------------------- | -------------- | --------------------- |
| Stable data         | 0.01 - 0.1     | Avoid false positives |
| Some drift expected | 0.002 - 0.01   | Default range         |
| Frequent drift      | 0.0005 - 0.002 | Quick detection       |
| Mission critical    | 0.0001 - 0.001 | Don't miss drift      |
| Batch training only | 0.1+           | Effectively disabled  |

---

## ğŸ¨ Use Case Optimization Guide

### ğŸ“Š **Time Series Forecasting**

```typescript
const timeSeriesModel = new MultivariatePolynomialRegression({
  polynomialDegree: 2, // Capture trends and seasonality
  learningRate: 0.005, // Moderate learning
  warmupSteps: 50, // Quick warmup
  regularizationStrength: 1e-3, // Prevent overfitting
  outlierThreshold: 2.5, // Handle anomalies
  adwinDelta: 0.001, // Detect trend changes
});
```

### ğŸ¤– **Sensor Data Processing**

```typescript
const sensorModel = new MultivariatePolynomialRegression({
  polynomialDegree: 3, // Capture non-linear sensor responses
  learningRate: 0.01, // Fast adaptation
  warmupSteps: 20, // Minimal warmup
  batchSize: 16, // Small batches for streaming
  outlierThreshold: 3.0, // Filter sensor glitches
  adwinDelta: 0.002, // Detect sensor drift
});
```

### ğŸ“ˆ **Financial Modeling**

```typescript
const financeModel = new MultivariatePolynomialRegression({
  polynomialDegree: 2, // Keep it simple
  learningRate: 0.001, // Stable learning
  warmupSteps: 200, // Long warmup
  totalSteps: 50000, // Extended training
  regularizationStrength: 1e-2, // Strong regularization
  outlierThreshold: 2.5, // Handle volatility spikes
  adwinDelta: 0.0005, // Sensitive to regime changes
});
```

### ğŸ­ **Industrial Process Control**

```typescript
const processModel = new MultivariatePolynomialRegression({
  polynomialDegree: 3, // Complex process dynamics
  learningRate: 0.005, // Moderate adaptation
  beta1: 0.8, // Faster momentum adaptation
  regularizationStrength: 1e-3, // Balance fit and stability
  convergenceThreshold: 1e-5, // High precision
  adwinDelta: 0.001, // Detect process changes
});
```

### ğŸ® **Real-time Gaming/Simulation**

```typescript
const realtimeModel = new MultivariatePolynomialRegression({
  polynomialDegree: 2, // Speed over complexity
  learningRate: 0.02, // Fast learning
  warmupSteps: 10, // Minimal warmup
  totalSteps: 1000, // Short horizon
  batchSize: 8, // Small batches
  convergenceThreshold: 1e-3, // Quick convergence
  outlierThreshold: 4.0, // Lenient filtering
});
```

### ğŸ”¬ **Scientific Research**

```typescript
const researchModel = new MultivariatePolynomialRegression({
  polynomialDegree: 5, // High flexibility
  learningRate: 0.0005, // Very stable
  warmupSteps: 500, // Long warmup
  totalSteps: 100000, // Extended training
  beta2: 0.9999, // Stable variance
  regularizationStrength: 1e-5, // Minimal bias
  convergenceThreshold: 1e-8, // High precision
});
```

---

## ğŸ“ Mathematical Background

### Polynomial Feature Expansion

For input vector **x** = [xâ‚, xâ‚‚, ..., xâ‚™] and degree d:

```
Ï†(x) = [1, xâ‚, xâ‚‚, ..., xâ‚™, xâ‚Â², xâ‚xâ‚‚, ..., xâ‚áµˆ, ..., xâ‚™áµˆ]
```

### Model Equation

```
Å· = W Â· Ï†(x)
```

Where W âˆˆ â„^(mÃ—k), m = output dimension, k = feature count

### Loss Function

```
L = (1/2n) Î£ ||y - Å·||Â² + (Î»/2) ||W||Â²
    \_____________________/   \________/
           MSE Loss         L2 Regularization
```

### Adam Update Rules

```
m_t = Î²â‚ Â· m_{t-1} + (1 - Î²â‚) Â· g_t       # First moment
v_t = Î²â‚‚ Â· v_{t-1} + (1 - Î²â‚‚) Â· g_tÂ²      # Second moment
mÌ‚_t = m_t / (1 - Î²â‚áµ—)                     # Bias correction
vÌ‚_t = v_t / (1 - Î²â‚‚áµ—)                     # Bias correction
W_t = W_{t-1} - Î· Â· mÌ‚_t / (âˆšvÌ‚_t + Îµ)     # Update
```

---

## âš¡ Performance Tips

### ğŸ’¾ Memory Optimization

```typescript
// For memory-constrained environments:
const memoryEfficientModel = new MultivariatePolynomialRegression({
  polynomialDegree: 2, // Fewer features
  batchSize: 16, // Smaller batches
});

// Call reset() when done to free memory
model.reset();
```

### ğŸš€ Speed Optimization

```typescript
// For maximum speed:
const fastModel = new MultivariatePolynomialRegression({
  polynomialDegree: 1, // Minimal features
  warmupSteps: 0, // No warmup
  convergenceThreshold: 1e-3, // Quick convergence
  batchSize: 128, // Larger batches
});
```

### ğŸ“Š Accuracy Optimization

```typescript
// For maximum accuracy:
const accurateModel = new MultivariatePolynomialRegression({
  polynomialDegree: 4, // More features
  learningRate: 0.0005, // Stable learning
  warmupSteps: 500, // Long warmup
  totalSteps: 100000, // Extended training
  convergenceThreshold: 1e-8, // High precision
  regularizationStrength: 1e-5, // Minimal bias
});
```

---

## ğŸ“„ License

MIT License - feel free to use in personal and commercial projects.

---

<div align="center">

**Made with â¤ï¸ for the machine learning community**

[â¬† Back to Top](#-multivariatepolynomialregression)

</div>
