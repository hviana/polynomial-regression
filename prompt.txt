Model: Implement TypeScript library "MultivariatePolynomialRegression": multivariate polynomial regression with incremental online learning, Adam optimizer, z-score normalization.

PERFORMANCE:
- Minimize memory allocations by reusing arrays and objects wherever possible
- Use typed arrays (Float64Array) for all numerical computations
- Avoid creating intermediate arrays in hot paths
- Implement in-place matrix operations
- Use object pooling for frequently created objects
- Minimize garbage collection pressure
- Cache computed values that are reused
- Use efficient loop structures (avoid forEach, map, reduce in critical paths)
- Preallocate buffers for polynomial feature generation
- Implement lazy initialization where appropriate
- Use iterative methods to prevent memory overallocation
- Write code that is highly CPU-optimized as well

DESIGN:
- Full numerical stability; OOP with interfaces for public contracts
- Private field encapsulation; JSDoc (@param, @returns, @example)
- Inline math formula docs; normalize inputs; track running average loss for accuracy

CONFIG (defaults):
polynomialDegree: 2 (1-10), learningRate: 0.001, warmupSteps: 100, totalSteps: 10000, beta1: 0.9, beta2: 0.999, epsilon: 1e-8, regularizationStrength: 1e-4, batchSize: 32, convergenceThreshold: 1e-6, outlierThreshold: 3.0, adwinDelta: 0.002

API:
fitOnline({ xCoordinates: number[][], yCoordinates: number[][] }): FitResult → Incremental Adam, Welford's z-score, L2 reg, outlier downweighting, ADWIN drift detection
fitBatch({ xCoordinates: number[][], yCoordinates: number[][], epochs?: number }): BatchFitResult → Mini-batch GD, shuffling, early stopping
predict(futureSteps: number): PredictionResult → Predictions
getModelSummary(): ModelSummary | getWeights(): WeightInfo | getNormalizationStats(): NormalizationStats | reset(): void

TYPES:
FitResult { loss, gradientNorm, effectiveLearningRate, isOutlier, converged, sampleIndex, driftDetected }
BatchFitResult { finalLoss, lossHistory: number[], converged, epochsCompleted, totalSamplesProcessed }
PredictionResult { predictions: SinglePrediction[], accuracy, sampleCount, isModelReady }
SinglePrediction { predicted, lowerBound, upperBound, standardError: number[] }
WeightInfo { weights, firstMoment, secondMoment: number[][], updateCount }
NormalizationStats { inputMean, inputStd, outputMean, outputStd: number[], count }
ModelSummary { isInitialized, inputDimension, outputDimension, polynomialDegree, polynomialFeatureCount, sampleCount, accuracy, converged, effectiveLearningRate, driftCount }

ALGORITHMS:

Adam + Cosine Warmup:
1. Normalize: x̃ = (x - μ)/(σ + ε)
2. Polynomial features φ: monomials ≤ degree d, graded lex order, count = C(n+d,d)
3. Forward: ŷ = W·φ, Loss = (1/2n)Σ‖y - ŷ‖²
4. Gradient: g = -(1/n)Σ(e ⊗ φ) + λW
5. LR: warmup → cosine decay
6. Update: m = β₁m + (1-β₁)g, v = β₂v + (1-β₂)g², W -= η(m/(1-β₁ᵗ))/(√(v/(1-β₂ᵗ)) + ε)

Welford: δ = x - μ, μ += δ/n, M₂ += δ(x - μ), σ² = M₂/(n-1) → inputs (normalize)

Accuracy: track running average loss L̄ = ΣLoss/n; accuracy = 1/(1 + L̄)

ADWIN Drift: adaptive error window; detect drift when |μ₀ - μ₁| ≥ εcut(δ); shrink window; reset stats on drift

Outliers: r = (y - ŷ)/σ; |r| > threshold → downweight 0.1×

Export: MultivariatePolynomialRegression
